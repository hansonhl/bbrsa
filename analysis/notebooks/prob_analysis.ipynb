{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Bert probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "from analysis_utils import *\n",
    "\n",
    "tokenizer, model, lm_model = prepare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_token_remap(src, tgt):\n",
    "    src_list = src.split()\n",
    "    print(src_list)\n",
    "    src_i = 0\n",
    "    cont = ''\n",
    "    res = []\n",
    "    for tgt_i, tok in enumerate(tgt):\n",
    "        if tok.startswith('##') and len(tok) > 2:\n",
    "            tok = tok.lstrip('##')\n",
    "        curr = cont + tok\n",
    "        assert src_list[src_i].startswith(curr), 'Mismatch in src and tgt! curr={} src={}'\\\n",
    "                .format(curr, src_list[src_i])\n",
    "        if curr == src_list[src_i]:\n",
    "            if curr == '[CLS]' or curr == '[SEP]':\n",
    "                res.append(None)\n",
    "            else:\n",
    "                res.append(src_i - 1)\n",
    "            src_i += 1\n",
    "            cont = ''\n",
    "        else:\n",
    "            res.append(src_i - 1)\n",
    "            cont = curr\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ['turkmen president gurbanguly berdymukhammedov will begin a two-day visit to russia , his country \\'s main energy partner , on monday for trade talks , the kremlin press office said .']\n",
    "in_tensor, str_toks, attn_mask, scrm_idxs = batch_to_idx_tensor(tokenizer, src)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lm_unmasked_output = lm_model(in_tensor, attention_mask=attn_mask)\n",
    "unmasked_scores = lm_unmasked_output[0].squeeze(0)[1:-1]\n",
    "# scores are to be fed into softmax to get prob. scores are already in log space.\n",
    "# use the following to get prob in log space\n",
    "unmasked_log_probs = unmasked_scores - torch.logsumexp(unmasked_scores, dim=1, keepdim=True)\n",
    "\n",
    "# now mask each token in src: make batch with same size as src len\n",
    "mask_in_tensor = in_tensor.repeat_interleave(in_tensor.shape[1] - 2, dim=0)\n",
    "attn_mask = attn_mask.repeat_interleave(in_tensor.shape[1] - 2, dim=0)\n",
    "mask_token = tokenizer.mask_token\n",
    "mask_id = tokenizer.convert_tokens_to_ids([mask_token])[0]\n",
    "r = torch.arange(mask_in_tensor.shape[0])\n",
    "idxs = r + 1\n",
    "mask_in_tensor[r, idxs] = mask_id\n",
    "\n",
    "with torch.no_grad():\n",
    "    lm_masked_output = lm_model(mask_in_tensor, attention_mask=attn_mask)\n",
    "    \n",
    "masked_scores = lm_masked_output[0][r, idxs]\n",
    "masked_log_probs = masked_scores - torch.logsumexp(masked_scores, dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the relationship between surprisal,  KL-divergence, entropy\n",
    "\n",
    "For each position in a sentence $X = x_1x_2...x_N$, I get two probabilities. \n",
    "\n",
    "One is the 'unmasked probability', $P_i(w_i \\mid X)$ which is the distribution over words at position $i \\in [1, N]$, given all words in the sentence, including $x_i$ itself.\n",
    "\n",
    "Another is the 'masked probability', $Q_i(w_i \\mid X_{-i})$, which is the distribution over words at position $i$, given all words in the sentence except that the word $x_i$ is masked.\n",
    "\n",
    "The surprisal of the word is calculated as $S = -\\log Q_i(x_i \\mid X_{-i})$.\n",
    "\n",
    "For each position $i$, the KL-divergence between the masked and unmasked probabilities is calculated: $KL(P_i, Q_i) = \\sum_{w \\in \\mathcal{V}}P_i(w)\\log\\left(\\frac{P_i(w)}{Q_i(w)}\\right)$. This is interpreted as a measure of information loss before and after masking the word $x_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(a, b):\n",
    "    \"\"\"Calculate KL divergence for a, b in logspace\"\"\"\n",
    "    return (a.exp() * (a - b)).sum(dim=-1)\n",
    "\n",
    "def entropy(a):\n",
    "    \"\"\"Calculate entropy given a with shape [len, vocab_size] for a in logspace\"\"\"\n",
    "    return (-a).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  mS  umS  KL  umH  mH\n",
      "turk  0.002079  3.338  11.49  4.377e+05  6.775e+05\n",
      "##men  0.04684  0.924  4.896  5.049e+05  6.351e+05\n",
      "president  0.113  0.004902  0.09585  7.32e+05  5.815e+05\n",
      "gu  0.2113  0.0002594  0.2096  8.256e+05  6.758e+05\n",
      "##rba  0.1048  0.01257  0.1196  6.323e+05  5.456e+05\n",
      "##ng  0.4632  0.1022  0.8444  6.098e+05  6.474e+05\n",
      "##ul  0.207  0.001814  0.2031  7.347e+05  5.528e+05\n",
      "##y  0.01636  0.004824  0.01823  7.039e+05  6.489e+05\n",
      "be  2.118  0.007103  2.108  7.196e+05  5.601e+05\n",
      "##rdy  7.206  1.483  7.132  5.238e+05  5.907e+05\n",
      "##mu  3.437  0.5322  2.364  5.481e+05  4.507e+05\n",
      "##kha  3.781  0.06216  3.519  6.344e+05  4.818e+05\n",
      "##mme  5.122  1.032  3.891  5.05e+05  5.316e+05\n",
      "##do  0.211  0.0001869  0.2105  8.672e+05  6.227e+05\n",
      "##v  0.0167  0.02242  0.08604  6.9e+05  6.602e+05\n",
      "will  0.8513  0.08677  0.5482  7.614e+05  5.678e+05\n",
      "begin  6.972  0.8514  5.233  5.197e+05  5.574e+05\n",
      "a  0.1463  0.0001774  0.145  8.95e+05  6.56e+05\n",
      "two  1.419  0.00124  1.41  8.643e+05  5.547e+05\n",
      "-  0.0001984  0.00123  0.005247  8.274e+05  6.718e+05\n",
      "day  0.8007  0.02385  0.8042  6.543e+05  6.318e+05\n",
      "visit  0.8854  0.004797  0.8737  7.162e+05  6.02e+05\n",
      "to  0.01921  1.907e-06  0.01919  9.688e+05  6.386e+05\n",
      "russia  1.555  0.002632  1.554  8.15e+05  5.392e+05\n",
      ",  0.001499  5.722e-06  0.001467  9.548e+05  6.917e+05\n",
      "his  4.323  0.00708  4.252  8.026e+05  6.271e+05\n",
      "country  0.1045  0.009218  0.0741  7.398e+05  6.572e+05\n",
      "'  0.0001507  3.815e-05  0.0002458  9.744e+05  8.485e+05\n",
      "s  -0.0  6.866e-05  0.0007175  9.004e+05  1.071e+06\n",
      "main  1.264  0.1989  0.7613  6.534e+05  5.017e+05\n",
      "energy  7.249  0.01714  7.177  6.76e+05  5.887e+05\n",
      "partner  2.381  0.008369  2.325  6.871e+05  5.769e+05\n",
      ",  0.003639  1.335e-05  0.003579  8.411e+05  6.238e+05\n",
      "on  0.2564  0.0007706  0.254  8.666e+05  5.604e+05\n",
      "monday  4.063  0.03129  3.874  7.514e+05  4.671e+05\n",
      "for  1.154  0.01785  1.102  9.03e+05  5.063e+05\n",
      "trade  3.848  0.0009518  3.841  7.325e+05  4.722e+05\n",
      "talks  1.626  0.1976  1.035  6.732e+05  4.955e+05\n",
      ",  0.004071  9.537e-06  0.004024  8.096e+05  6.464e+05\n",
      "the  0.02646  1.717e-05  0.02635  8.441e+05  5.988e+05\n",
      "k  0.0002213  0.01099  0.07851  7.109e+05  7.888e+05\n",
      "##rem  0.0006294  0.0001087  0.0007952  7.931e+05  7.059e+05\n",
      "##lin  0.001055  4.722  14.05  5.31e+05  7.608e+05\n",
      "press  2.41  0.01146  2.382  7.661e+05  4.945e+05\n",
      "office  2.258  0.007718  2.218  7.265e+05  5.404e+05\n",
      "said  0.9173  0.04407  0.8689  6.644e+05  4.617e+05\n",
      ".  0.0008907  3.815e-06  0.000882  9.817e+05  8.075e+05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kl = kl_divergence(unmasked_log_probs, masked_log_probs)\n",
    "unmasked_entropy = entropy(unmasked_log_probs)\n",
    "masked_entropy = entropy(masked_log_probs)\n",
    "\n",
    "word_ids = in_tensor[0, 1:-1]\n",
    "masked_surprisal = -masked_log_probs[r, word_ids]\n",
    "unmasked_surprisal = -unmasked_log_probs[r, word_ids]\n",
    "\n",
    "word_strs = str_toks[0][1:-1]\n",
    "print('Word  mS  umS  KL  umH  mH')\n",
    "for word, ms, ums, k, ume, me in zip(word_strs, masked_surprisal, unmasked_surprisal, kl, unmasked_entropy, masked_entropy):\n",
    "    print('{}  {:.4}  {:.4}  {:.4}  {:.4}  {:.4}'.format(word, ms, ums, k, ume, me))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding efficient methods to retrieve words with high surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['energy', '##rdy', 'begin', '##mme', 'his', 'monday', 'trade', '##kha', '##mu', 'press']\n",
      "['##lin', 'turk', '##rdy', '##mme', '##men', 'begin', '##mu', 'main', 'talks', '##ng']\n",
      "Precision 0.4, recall 0.4\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict as DD\n",
    "\n",
    "def precision_and_recall(tgt, hyp):\n",
    "    tgt_dict = DD(int)\n",
    "    tgt_size = len(tgt) if isinstance(tgt, list) else tgt.shape[0]\n",
    "    for y in tgt:\n",
    "        tgt_dict[y] += 1\n",
    "    hyp_size = len(hyp) if isinstance(hyp, list) else hyp.shape[0]\n",
    "    overlap = 0\n",
    "    for x in hyp:\n",
    "        if x in tgt_dict:\n",
    "            overlap += 1\n",
    "    precision = overlap / hyp_size\n",
    "    recall = overlap / tgt_size\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "_, tgt_top_idxs = masked_surprisal.topk(10)\n",
    "tgt_tok_strs = [word_strs[i] for i in tgt_top_idxs]\n",
    "print(tgt_tok_strs)\n",
    "\n",
    "_, hyp_top_idxs = unmasked_surprisal.topk(10)\n",
    "hyp_tok_strs = [word_strs[i] for i in hyp_top_idxs]\n",
    "print(hyp_tok_strs)\n",
    "\n",
    "prec, recl = precision_and_recall(tgt_tok_strs, hyp_tok_strs)\n",
    "\n",
    "print('Precision {:.4}, recall {:.4}'.format(prec, recl))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 49, 49])\n",
      "torch.Size([47])\n",
      "['turk', 'monday', 'russia', '##kha', 'talks', 'visit', 'partner', 'begin', 'president', '##rdy']\n",
      "Precision 0.4, recall 0.4\n"
     ]
    }
   ],
   "source": [
    "in_tensor, str_toks, attn_mask, scrm_idxs = batch_to_idx_tensor(tokenizer, src)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_res = model(in_tensor, attention_mask=attn_mask)\n",
    "    \n",
    "attns = model_res[2]\n",
    "layer0_attns = attns[0]\n",
    "print(layer0_attns.shape)\n",
    "summed = layer0_attns.sum(dim=2).sum(dim=1)\n",
    "summed = (summed / summed.sum(dim=1)).squeeze(0)[1:-1]\n",
    "print(summed.shape)\n",
    "_, hyp_top_idxs = summed.topk(10)\n",
    "hyp_tok_strs = [word_strs[i] for i in hyp_top_idxs]\n",
    "print(hyp_tok_strs)\n",
    "\n",
    "prec, recl = precision_and_recall(tgt_tok_strs, hyp_tok_strs)\n",
    "\n",
    "print('Precision {:.4}, recall {:.4}'.format(prec, recl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
