{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import time\n",
    "import sys, os\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "ONMT_DIR = '../myOpenNMT'\n",
    "sys.path.append(os.path.abspath(ONMT_DIR))\n",
    "\n",
    "from bbrsa import ONMTSummaryRSA\n",
    "from models import ONMTSummarizer\n",
    "from pragmatics import NextExampleDistractor, IdenticalDistractor\n",
    "from pragmatics import BasicPragmatics, MemoizedListener\n",
    "from utils import init_logger, display\n",
    "\n",
    "# logger = init_logger(no_format=True, print_level=logging.WARNING, log_file='logs/log5.txt',\n",
    "#                      log_file_level=logging.WARNING, log_mode='w')\n",
    "logger = init_logger(no_format=True, print_level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = 'data/giga_small_input.txt'\n",
    "tgt_file = 'data/giga_small_target.txt'\n",
    "with open(src_file, 'r') as f:\n",
    "    big_src = f.readlines()\n",
    "with open(tgt_file, 'r') as f:\n",
    "    big_tgt = f.readlines()\n",
    "src = big_src\n",
    "tgt = big_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring summary model...\n",
      "Import successful\n",
      "Finished configuration.\n",
      "\n",
      "==== Beginning Summary with distractor ====\n",
      "/home/hansonlu/anaconda2/envs/opennmt/lib/python3.7/site-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "==== Beginning Summary with S0 ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: nec UNK computer to join forces in supercomputer sales\n",
      "S0: nec UNK computer to join forces in supercomputer sales\n",
      "\n",
      "S1: sri lanka announces closure of schools\n",
      "S0: sri lanka closes schools with immediate effect on tamil rebels\n",
      "\n",
      "S1: police arrest five anti-nuclear protesters after trying to loading french research ship\n",
      "S0: police arrest five anti-nuclear protesters\n",
      "\n",
      "S1: factory orders up #.# percent in september\n",
      "S0: factory orders up #.# percent in september\n",
      "\n",
      "S1: boj urges calm after daiwa bank us deal\n",
      "S0: boj urges calm after daiwa bank closure\n",
      "\n",
      "S1: croatian croatian negotiators agree to meet\n",
      "S0: croatian croatian negotiators to meet saturday on last serb-held area\n",
      "\n",
      "S1: toyota europe banned from world rally championship\n",
      "S0: toyota europe banned from world rally championship\n",
      "\n",
      "S1: israel prepares for rabin funeral\n",
      "S0: israel prepares for rabin funeral\n",
      "\n",
      "S1: indian pm 's promise of autonomy in promise\n",
      "S0: indian pm 's promise of autonomy for kashmir sparks violent reaction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "giga_config_path = 'giga_inference.yml'\n",
    "s0 = ONMTSummarizer(config_path=giga_config_path, logger=logger)\n",
    "pragmatics = MemoizedListener(alpha=1, logger=logger)\n",
    "distractor = NextExampleDistractor(batch_size=s0.opt.batch_size, logger=logger)\n",
    "model = ONMTSummaryRSA(s0, pragmatics, distractor, logger=logger)\n",
    "\n",
    "pred1 = model.summarize_with_distractor(src, beam_size=1)\n",
    "pred2 = model.summarize_with_s0(src, beam_size=1)\n",
    "\n",
    "display(['S1', 'S0'], [pred1, pred2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting on the Gigaword dataset\n",
    "\n",
    "$\\alpha = 5$\n",
    "__(distractor)__\n",
    "police arrested __six anti-government__ protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .\n",
    "\n",
    "S1: six anti-government protesters arrested in france\n",
    "\n",
    "S0: police arrest six protesters in attempt to disrupt antarctic research\n",
    "\n",
    "__(original text)__\n",
    "police arrested __five anti-nuclear__ protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .\n",
    "\n",
    "S1: five anti-nuclear protesters arrested\n",
    "\n",
    "S0: police arrest five anti-nuclear protesters\n",
    "\n",
    "__(reference)__ protesters target french research ship\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==== Beginning Summary with distractor ====\n",
      "==== Beginning Summary with S0 ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: six anti-government protesters arrested in france\n",
      "S0: police arrest six protesters in attempt to disrupt antarctic research\n",
      "\n",
      "S1: five anti-nuclear protesters arrested\n",
      "S0: police arrest five anti-nuclear protesters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src = ['police arrested six anti-government protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .',\n",
    "       'police arrested five anti-nuclear protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .']\n",
    "\n",
    "pragmatics = BasicPragmatics(alpha=5, logger=logger)\n",
    "model = ONMTSummaryRSA(s0, pragmatics, distractor, logger=logger)\n",
    "s1_pred = model.summarize_with_distractor(src, beam_size=1)\n",
    "s0_pred = model.summarize_with_s0(src, beam_size=1)\n",
    "\n",
    "display(['S1', 'S0'], [s1_pred, s0_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__(distractor)__ police arrested five anti-nuclear protesters friday after they sought to disrupt __operations at a nuclear plant__ , a spokesman for the protesters said .\n",
    "\n",
    "S1($\\alpha = 1$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 2$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 5$): nuclear plant protesters arrested\n",
    "    \n",
    "S1($\\alpha = 10$): nuclear plant operations shut \n",
    "\n",
    "S0: police arrest five anti-nuclear protesters\n",
    "\n",
    "__(original)__ police arrested five anti-nuclear protesters friday after they sought to disrupt __loading of a french antarctic research and supply vessel__ , a spokesman for the protesters said .\n",
    "\n",
    "S1($\\alpha = 1$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 2$): anti-nuclear protesters arrested in france\n",
    "\n",
    "S1($\\alpha = 5$): antarctic research vessel arrested after french ship blocked\n",
    "\n",
    "S1($\\alpha = 10$): antarctic research vessel arrested over french ship\n",
    "\n",
    "S0: police arrest five anti-nuclear protesters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==== Beginning Summary with distractor ====\n",
      "==== Beginning Summary with S0 ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: nuclear plant protesters arrested\n",
      "S0: police arrest five anti-nuclear protesters\n",
      "\n",
      "S1: anti-nuclear protesters arrested in france\n",
      "S0: police arrest five anti-nuclear protesters\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src = ['police arrested five anti-nuclear protesters friday after they sought to disrupt operations at a nuclear plant , a spokesman for the protesters said .',\n",
    "       'police arrested five anti-nuclear protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .']\n",
    "\n",
    "model.set_alpha(3)\n",
    "s1_pred = model.summarize_with_distractor(src, beam_size=1)\n",
    "s0_pred = model.summarize_with_s0(src, beam_size=1)\n",
    "\n",
    "display(['S1', 'S0'], [s1_pred, s0_pred])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(distractor)__ police arrested five anti-nuclear protesters friday after they sought to disrupt loading of __an american__ antarctic research and supply vessel , a spokesman for the protesters said .\n",
    "\n",
    "S0, S1($\\alpha = 1$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 2$): police arrest five protesters in attempt to disrupt u.s. antarctic research\n",
    "\n",
    "S1($\\alpha = 3$): police arrest five protesters in anti-whaling protest\n",
    "\n",
    "S1($\\alpha = 4$): police arrest five protesters\n",
    "\n",
    "S1($\\alpha = 5$): american antarctic protesters arrested\n",
    "\n",
    "S1($\\alpha = 10$): american antarctic protesters arrested\n",
    "\n",
    "__(original)__ police arrested five anti-nuclear protesters friday after they sought to disrupt loading of __a french__ antarctic research and supply vessel , a spokesman for the protesters said .\n",
    "\n",
    "S0, S1($\\alpha = 1$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 2$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 3$): police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 4$): french police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 5$): french police arrest five anti-nuclear protesters\n",
    "\n",
    "S1($\\alpha = 10$): french antarctic protesters arrested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==== Beginning Summary with distractor ====\n",
      "==== Beginning Summary with S0 ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: climate change protesters arrested at french antarctic research\n",
      "S0: police arrest five climate change protesters\n",
      "\n",
      "S1: anti-nuclear protesters arrested\n",
      "S0: police arrest five anti-nuclear protesters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src = ['police arrested five climate change protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .',\n",
    "       'police arrested five anti-nuclear protesters friday after they sought to disrupt loading of a french antarctic research and supply vessel , a spokesman for the protesters said .']\n",
    "\n",
    "model.set_alpha(4)\n",
    "s1_pred = model.summarize_with_distractor(src, beam_size=1)\n",
    "s0_pred = model.summarize_with_s0(src, beam_size=1)\n",
    "\n",
    "display(['S1', 'S0'], [s1_pred, s0_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying greater number of distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'b', 'c', 'd', 'e', 'f', 'b', 'c', 'd', 'e', 'f', 'g', 'c', 'd', 'e', 'f', 'g', 'a', 'd', 'e', 'f', 'g', 'a', 'b', 'e', 'f', 'g', 'a', 'b', 'c', 'f', 'g', 'a', 'b', 'c', 'd', 'g', 'a', 'b', 'c', 'd', 'e'], 120)\n",
      "Configuring summary model...\n",
      "Import successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==== Beginning Summary with distractor ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished configuration.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==== Beginning Summary with S0 ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 finished, duration: 2.741589307785034\n"
     ]
    }
   ],
   "source": [
    "from pragmatics import NextNDistractor\n",
    "\n",
    "testsrc = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n",
    "\n",
    "testdistractor = NextNDistractor(batch_size=20, N=5)\n",
    "print(testdistractor.generate(testsrc))\n",
    "\n",
    "s0 = ONMTSummarizer(config_path=giga_config_path)\n",
    "pragmatics = BasicPragmatics(alpha=3)\n",
    "distractor = NextNDistractor(batch_size=20, N=5)\n",
    "\n",
    "model4 = ONMTSummaryRSA(s0, pragmatics, distractor)\n",
    "\n",
    "start_time = time.time()\n",
    "s1_pred = model.summarize_with_distractor(big_src, beam_size=10)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print('s1 finished, duration:', duration)\n",
    "s0_pred = model.summarize_with_s0(big_src, beam_size=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using memoized L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Beginning Summary with distractor ====\n",
      "==== Beginning Summary with S0 ====\n",
      "S1: anti-whaling protesters arrested after whaling protest\n",
      "S0: anti-whaling protesters arrested in tokyo\n",
      "\n",
      "S1: greenpeace backs greenpeace protests against russian blockade\n",
      "S0: greenpeace backs anti-nuclear protesters\n",
      "\n",
      "S1: police arrest five protesters in attempt to disrupt us research ship\n",
      "S0: police arrest five anti-nuclear protesters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pragmatics = MemoizedListener(alpha=1)\n",
    "distractor = NextNDistractor(batch_size=s0.opt.batch_size, N=2)\n",
    "model = ONMTSummaryRSA(s0, pragmatics, distractor)\n",
    "\n",
    "src = ['police arrested five anti-whaling protesters friday after they sought to disrupt loading of a japanese whaling research and supply vessel , a spokesman for the protesters said .',\n",
    "       'greenpeace supported five anti-nuclear protesters friday after they sought to disrupt loading of a russian antarctic research and supply vessel , a spokesman for the protesters said .',\n",
    "       'police arrested five anti-nuclear protesters friday after they sought to disrupt loading of an american antarctic research and supply vessel , a spokesman for the protesters said .']\n",
    "\n",
    "\n",
    "s1_pred = model.summarize_with_distractor(src, beam_size=1)\n",
    "s0_pred = model.summarize_with_s0(src, beam_size=1)\n",
    "\n",
    "display(['S1', 'S0'], [s1_pred, s0_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to understand what is the model in the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aan_useffn=False\n",
      "accum_count=1\n",
      "adagrad_accumulator_init=0\n",
      "adam_beta1=0.9\n",
      "adam_beta2=0.999\n",
      "apex_opt_level='O2'\n",
      "audio_enc_pooling='1'\n",
      "batch_size=64\n",
      "batch_type='sents'\n",
      "bridge=False\n",
      "brnn=False\n",
      "brnn_merge='concat'\n",
      "cnn_kernel_width=3\n",
      "context_gate=None\n",
      "copy_attn=True\n",
      "copy_attn_force=False\n",
      "copy_attn_type=None\n",
      "copy_loss_by_seqlength=False\n",
      "coverage_attn=False\n",
      "data='../data/raw_shuffled/data'\n",
      "dec_layers=2\n",
      "dec_rnn_size=500\n",
      "decay_method=''\n",
      "decoder_type='rnn'\n",
      "dropout=0.3\n",
      "enc_layers=2\n",
      "enc_rnn_size=500\n",
      "encoder_type='rnn'\n",
      "epochs=20\n",
      "exp=''\n",
      "exp_host=''\n",
      "feat_merge='concat'\n",
      "feat_vec_exponent=0.7\n",
      "feat_vec_size=-1\n",
      "fix_word_vecs_dec=False\n",
      "fix_word_vecs_enc=False\n",
      "generator_function='softmax'\n",
      "global_attention='general'\n",
      "global_attention_function='softmax'\n",
      "gpuid=[0]\n",
      "heads=8\n",
      "input_feed=1\n",
      "label_smoothing=0.0\n",
      "lambda_coverage=1\n",
      "layers=-1\n",
      "learning_rate=1.0\n",
      "learning_rate_decay=0.5\n",
      "loss_scale=0\n",
      "max_generator_batches=32\n",
      "max_grad_norm=5\n",
      "max_relative_positions=0\n",
      "model_dtype='fp32'\n",
      "model_type='text'\n",
      "normalization='sents'\n",
      "optim='sgd'\n",
      "param_init=0.1\n",
      "position_encoding=False\n",
      "pre_word_vecs_dec=None\n",
      "pre_word_vecs_enc=None\n",
      "report_every=50\n",
      "reuse_copy_attn=True\n",
      "rnn_size=500\n",
      "rnn_type='LSTM'\n",
      "sample_rate=16000\n",
      "save_model='../data/raw_shuffled/model-copy'\n",
      "seed=-1\n",
      "self_attn_type='scaled-dot'\n",
      "share_decoder_embeddings=False\n",
      "share_embeddings=False\n",
      "src_word_vec_size=500\n",
      "start_checkpoint_at=0\n",
      "start_decay_at=8\n",
      "start_epoch=1\n",
      "tgt_word_vec_size=500\n",
      "train_from=''\n",
      "transformer_ff=2048\n",
      "truncated_decoder=0\n",
      "valid_batch_size=32\n",
      "warmup_steps=4000\n",
      "window_size=0.02\n",
      "word_vec_size=-1\n"
     ]
    }
   ],
   "source": [
    "t = \"aan_useffn=False, accum_count=1, adagrad_accumulator_init=0, adam_beta1=0.9, adam_beta2=0.999, apex_opt_level='O2', audio_enc_pooling='1', batch_size=64, batch_type='sents', bridge=False, brnn=False, brnn_merge='concat', cnn_kernel_width=3, context_gate=None, copy_attn=True, copy_attn_force=False, copy_attn_type=None, copy_loss_by_seqlength=False, coverage_attn=False, data='../data/raw_shuffled/data', dec_layers=2, dec_rnn_size=500, decay_method='', decoder_type='rnn', dropout=0.3, enc_layers=2, enc_rnn_size=500, encoder_type='rnn', epochs=20, exp='', exp_host='', feat_merge='concat', feat_vec_exponent=0.7, feat_vec_size=-1, fix_word_vecs_dec=False, fix_word_vecs_enc=False, generator_function='softmax', global_attention='general', global_attention_function='softmax', gpuid=[0], heads=8, input_feed=1, label_smoothing=0.0, lambda_coverage=1, layers=-1, learning_rate=1.0, learning_rate_decay=0.5, loss_scale=0, max_generator_batches=32, max_grad_norm=5, max_relative_positions=0, model_dtype='fp32', model_type='text', normalization='sents', optim='sgd', param_init=0.1, position_encoding=False, pre_word_vecs_dec=None, pre_word_vecs_enc=None, report_every=50, reuse_copy_attn=True, rnn_size=500, rnn_type='LSTM', sample_rate=16000, save_model='../data/raw_shuffled/model-copy', seed=-1, self_attn_type='scaled-dot', share_decoder_embeddings=False, share_embeddings=False, src_word_vec_size=500, start_checkpoint_at=0, start_decay_at=8, start_epoch=1, tgt_word_vec_size=500, train_from='', transformer_ff=2048, truncated_decoder=0, valid_batch_size=32, warmup_steps=4000, window_size=0.02, word_vec_size=-1\"\n",
    "\n",
    "split_text = t.split(', ')\n",
    "for line in split_text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with abstract classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "\n",
    "class A(ABC):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def blablabla(self):\n",
    "        print('I shall say blablabla ' + str(self.value) + ' times')\n",
    "    \n",
    "class AA(A):\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value)\n",
    "    \n",
    "    @abstractproperty\n",
    "    def funky(self):\n",
    "        return self._funky\n",
    "    \n",
    "    @property\n",
    "    def super_funky(self):\n",
    "        return self.funky * self.value\n",
    "\n",
    "class B(AA):\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value)\n",
    "        self._funky = 100\n",
    "    \n",
    "    @property\n",
    "    def funky(self):\n",
    "        return self._funky\n",
    "\n",
    "b = B(1000)\n",
    "print(b.funky)\n",
    "print(b.super_funky)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
